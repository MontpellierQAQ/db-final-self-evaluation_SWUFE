# 数据库原理课程自我评价
## 42233058 张贤杰
### **写在前面：** 
时光如苒，日月如梭。转眼

#### 评分依据：计算每周得分平均分9.2分，转为50分制后为46分

  

  

  



### **第1周**（9.5分）

**课程内容：** 数据库系统概论（“是什么”与“为什么”）

**总体感受：**
以往我对“数据库”的理解非常模糊，可能就等同于一个能存很多数据的Excel表格，而课程重塑了我的理解。

**具体收获与疑难点反思：**

1. **初识数据模型与数据库语言**

   * **收获：** 学习到多种数据模型，重点是**关系模型（Relational Model）**，即用二维表格（关系）来组织数据。

   * **心得：** SQL作为**声明式语言**的特点让我感到新奇。与我之前学习的Python等命令式语言不同，只需要用SQL“声明”想要`WHAT`，而不需要关心`HOW`去获取。

2. **对这周任务的思考与实践**

   * **实践：** 通过B站视频学习，电脑上成功安装了PostgreSQL，并安装了DataGrip作为客户端工具，已成功连接到本地数据库。完成了作业内容和思考题：

   * **思考（“关系”的理解）：** 课后问题：“如何理解关系数据库的‘关系’？”。起初我以为“关系”指的是表与表之间的关联（比如学生和课程的选课关系）。但通过回顾课本内容，我意识到这里的“关系”更是一个数学概念，源于集合论。一个**表（Table）本身就是一个关系（Relation）**，表的每一行（Row）是一个**元组（Tuple）**，而整个表就是元组的集合。表之间的“关联”是在这个数学基础上建立起来的，这个认知上的纠正对我来说至关重要。


---

### **第2周**（9.5分）

**本周核心内容：** 关系代数

**总体感受：**

关系代数像是在学习一门新的逻辑语言，虽然抽象，但也让我理解到SQL查询的本质。

**具体收获与疑难点反思：**

1. **对“关系”的再认识：从表格到集合**

   * **收获：** 关系数据库中的“关系”在数学上是一个**元组的集合** 有两个重要特性：

     1. **元组不重复：** 作为一个集合，关系中不允许出现完全相同的元组（行）

      2. **元组无序：** 集合中的元素是无序的，因此元组的顺序无关紧要

   * **心得：** 这个认知纠正了我之前将数据库表等同于Excel工作表的直观想法。在Excel里，我可以有任意多行完全相同的数据，并且行的顺序是固定的。而数据库的“关系”本质上是无序且唯一的，这让我理解了为什么查询结果的顺序在不指定`ORDER BY`时是不保证的。

3. **理解“码”的层级：Super Key, Candidate Key, Primary Key, Foreign Key**

   * **收获：** 理清了各种“码”（Key）之间的关系

     * **超码 (Super Key):** 任何能唯一标识一行数据的属性组合。例如，`(身份证号, 姓名)` 就是一个超码。
     * **候选码 (Candidate Key):** “最小”的超码。即从该码中移除任何一个属性，它就不再是超码，一个关系中可能有多个候选码。
     * **主码 (Primary Key):** 被数据库设计者从所有候选码中**指定**的一个。
     * **外码 (Foreign Key):** 连接两个表的“纽带”。它是一个表中的属性，其值对应另一个表的主码。

4. **关系代数**

   * **收获：** 理清了关系代数的基本概念：

     * **选择 (σ):** 像筛子一样，按行进行筛选（水平方向）。例如 `σ_dept_name=“Physics”`。
     * **投影 (Π):** 像刀一样，按列进行切割（垂直方向）。例如 `Π_name,salary`。
     * **笛卡尔积 (×):** 将两个关系的所有元组进行暴力组合。
     * **连接 (⨝):** 等价于一个**笛卡尔积**后再跟一个**选择**。公式 `r ⨝_θ s = σ_θ(r × s)`
     * **自然连接 (Natural Join):** 自动在两个关系中所有同名属性上进行等值连接。
   * **心得：** “你可以在不懂理论的情况下成为一名数据库开发者，但你将永远局限于做你以前见过的事情。” ——学习关系代数的意义：它不仅是理论，更是数据库内部执行和优化SQL的语言。

**本周实践：**
本周的理论知识，尤其是关系代数部分，还需要通过实际练习来加深理解。我尝试将练习题中的自然语言需求，先在脑中构思成关系代数表达式，然后再去思考如何用SQL实现，以此来锻炼逻辑转换能力。

---

### **第3周**（9.5分）

**本周核心内容：** SQL入门——DDL（数据定义语言）与基础DML（数据操纵语言）

**具体收获与疑难点反思：**

1. **DDL**

   * **收获：** 掌握了使用 `CREATE TABLE` 语句来定义一个关系（表），关键要素包括：
   * **数据类型 :** 为每个属性规定存储什么类型的数据。
   * `char(n)`、`varchar(n)`和`numeric(p, d)`（Tips：用`numeric`比`float`或`double`更可靠，可以避免浮点数精度问题）
   * **完整性约束 :** 通过`PRIMARY KEY`、`FOREIGN KEY`、`NOT NULL`等约束，把第二周学的“码”和业务规则真正地固化到了数据库结构中。
   * **疑难点：** 表名用单数还是复数的问题——我个人更倾向于使用**单数**（如 `product`），因为关系（表）在概念上代表了一个**实体类型**的集合，`product` 表代表“产品”这个概念，而其中的每一行才是一个具体的产品实例。这种命名方式在逻辑上更清晰。

2. **DML**

   * **收获：** 我理解了最核心的SQL查询结构，并且养成了“先看`FROM`，再看`WHERE`，最后看`SELECT`”的阅读习惯。

     * `FROM`: 确定数据来源是哪些表。如果是多表，脑中会先形成一个笛卡尔积。
     * `WHERE`: 对笛卡尔积的结果进行行级筛选，过滤掉不满足条件的元组。这是关系代数中 **选择(σ)** 的体现。
     * `SELECT`: 决定最终展示哪些列。这是关系代数中 **投影(Π)** 的体现。
   * **心得：** 将SQL语句与上周的关系代数表达式对应起来，学习过程变得豁然开朗。例如：
     `select name from instructor where dept_name = 'Physics';`
     就等价于 `Π_name(σ_dept_name=“Physics”(instructor))`。
     这种对应关系让我明白，SQL不仅仅是一套命令，它背后有严谨的数学理论支撑。

3. **细节易错：**

   * **收获：** 整理了非常重要且容易出错的几个细节：

     * **字符串用单引号 `''`**：※※※这与很多编程语言用双引号的习惯不同，需要特别记忆※※※
     * **`DISTINCT` 关键字**：用于去除查询结果中的重复行。
     * **`AS` 别名**：在多表查询中，为表和列起别名。

4. **排序**
   * **收获：** `ORDER BY` 子句是SQL查询结果中唯一能保证顺序的方式。默认是升序（`ASC`），并且可以指定降序（`DESC`）。
   * **疑难点：** 对于 `ORDER BY A, B DESC` 的排序逻辑，我理解为：首先整体按照 **A列的升序** 进行排序；在A列值相同的情况下，这些值相同的行再按照 **B列的降序** 进行内部排序（这个多级排序逻辑在实际报表需求中肯定会非常常用）

**本周实践：**
本周的学习内容实践性非常强，课后我进行了如下实践：
1. 反复练习`SELECT-FROM-WHERE`查询，特别是多表连接查询，熟练掌握`WHERE`子句中连接条件的写法。
   
2. 完成本周的作业，并尝试对每个查询都先思考其关系代数表达式。


---

### **第4周（第一次实操）** （9.5分）

**具体收获与疑难点反思：**

1. **字符串操作**

   * **收获：** 学习了SQL中处理字符串的各种技巧，特别是模糊查询`LIKE`，总结归纳如下：

     * **通配符：** `%`（匹配任意长度字符串） `_`（匹配单个任意字符）
     * **转义：** 例如`LIKE 'abc\%d%'`。
     * **函数：** `lower()`、`upper()`、`trim()`、`length()` 
     * **拼接：** 了解了不同数据库方言在字符串拼接上的差异（PG用`||`，MySQL用`concat()`等），这让我第一次直观地感受到了“SQL方言”的存在。

   * **疑难点整理：**

     * **单引号转义：** `I'm good` 在SQL中要写成 `I''m good`，即用两个单引号来转义一个单引号。我一开始误以为是双引号，经过老师的强调和自己动手尝试才彻底记住，这是一个非常容易踩的坑。
       
     * **`LIKE`的逻辑：** `name LIKE 'Wu%'`和`name LIKE 'Wu_'`的区别——前者会匹配到'Wu'，而后者不会。这让我意识到`%`可以匹配空字符串，而`_`必须匹配一个确切的字符。这个细节对于精确控制模糊查询的范围至关重要。

2. **集合操作：关系代数的SQL实现**

   * **收获：** 了解了可以使用`UNION`、`INTERSECT`和`EXCEPT`来进行集合运算，这完美地对应了第二周学习的关系代数中的并(∪)、交(∩)、差(-)运算。
     * 整理了以下不常用的集合操作：
     * `UNION`: 合并两个查询结果，并自动去重（`UNION ALL`则不去重）。
     * `INTERSECT`: 取两个查询结果的交集。
     * `EXCEPT`: 取第一个查询结果与第二个查询结果的差集。

    * **心得：**  **SQL方言**的问题：在实际工作中，必须清楚自己所使用的支持哪些特性。不能想当然地认为在一个平台上能跑的SQL在另一个平台上也一定能跑。
   

---

### **第5周** （9.5分）

**核心内容：** SQL进阶——空值（NULL）的特性、聚合函数与分组查询、嵌套子查询

**总体感受：**
本周的学习曲线明显变陡，从基础的“增删查改”迈入了真正的数据分析领域。`NULL`值的“反直觉”逻辑、`GROUP BY`的分组思想以及层层嵌套的子查询，锻炼了我的逻辑思维能力。
**具体收获与疑难点反思：**

1. **重新认识`NULL`：它不是值，是“未知”状态**

   * **收获：** 本周我学到的第一个“颠覆性”知识点是`NULL`的行为。`NULL`不等于0，不等于空字符串，它代表“未知”。这个核心定义解释了它所有的奇怪特性：

     * 任何涉及`NULL`的算术运算（`+`, `-`, `*`, `/`）结果都是`NULL`。
     * 任何与`NULL`的比较运算（`>`, `<`, `=`）结果都是`unknown`。在`WHERE`子句中，`unknown`和`false`一样，都会导致行被过滤掉。

    * **疑难点：** 例子 “Zhongpu老师的工资为`NULL`，他是否会出现在`salary < 80000`或`salary >= 80000`的查询结果中？” ———— 两个查询他都不会出现，因为两个条件都返回`unknown` **必须使用 `IS NULL` 或 `IS NOT NULL` 来判断空值**，而绝不能用 `= NULL`，因为`NULL = NULL`的结果依然是`unknown`。

3. **聚合与分组**

   * **收获：** 掌握了五大聚合函数（`AVG`, `SUM`, `COUNT`, `MAX`, `MIN`），并学会了使用 `GROUP BY` 子句。
   * **心得：** `GROUP BY` 相当于一种视角的转换，从关注每一条独立的记录，转变为关注具有相同特征的“群体”
   * **关键规则：** 我牢牢记住了那个重要提醒：“**出现在`SELECT`子句中但没有被聚合的属性，必须是`GROUP BY`子句中的属性**”。`SELECT dept_name, ID, avg(salary)`这个非法查询的例子让我深刻理解了其原因：对于一个`dept_name`分组（比如'Comp. Sci.'），它包含多个`ID`，数据库不知道该显示哪一个，因此这是不明确且非法的。

4. **`WHERE` 与 `HAVING` 的用法**

   * **收获：** 厘清了`WHERE`和`HAVING`的区别：

     * `WHERE`：**分组前**的行级过滤。它作用于原始的、未分组的表数据
     * `HAVING`：**分组后**的组级过滤。它作用于由`GROUP BY`生成的各个分组
   * **心得：** 后面回来翻阅看到PPT中的SQL执行次序图（FROM -> WHERE -> GROUP BY -> HAVING -> SELECT）这对我帮助巨大，它从直观意义上解释了为什么`HAVING`子句中可以使用聚合函数（如 `HAVING avg(salary) > 42000`），而`WHERE`子句中不能。

5. **子查询与`WITH`子句**

   * **收获：** 学习了（但还不熟练）多种嵌套子查询的用法。
     * 
     * **`IN`/`NOT IN`:** 用于集合成员资格判断，非常直观。
     * **`> SOME`/`> ALL`:** 这两个比较抽象。我的理解是：`> SOME`等价于“大于集合中的最小值”；`> ALL`等价于“大于集合中的最大值”。这样转换后就容易理解多了。
     * **`EXISTS`:** 它不关心子查询返回了什么，只关心“是否返回了结果”。老师提到的“当子查询结果集很大时，用`EXISTS`通常比`IN`更高效”是一个非常宝贵的实践经验。
     * **`WITH`子句 (CTE):** 它像编程中的定义临时变量或函数，能将一个非常复杂的、多层嵌套的查询，拆解成几个逻辑清晰的、命名的临时关系。Tips:课上对比了使用`WITH`和不使用`WITH`来查询“工资总额大于平均工资总额的系”

**本周实践：**
本周的内容需要练习来巩固。我完成了本周作业，重点练习了`GROUP BY`和`HAVING`的组合使用。

---

### **第6周** （8.5分）

**本周核心内容：** SQL：修改数据库

**总体感受：**
本周学习了如何“增、删、改”数据（`INSERT`, `DELETE`, `UPDATE`）以及如何动态地修改数据库的结构（`ALTER TABLE`）。这些操作都具有“破坏性”，让我深刻体会到在生产环境中操作数据库必须慎之又慎。

**具体收获与疑难点反思：**

1. **DML进阶：不只是增删改**

   * **收获：** 我掌握了`INSERT`, `DELETE`, `UPDATE`的基本语法。特别注意：

     * **`INSERT INTO ... SELECT ...`**: 允许将一个查询的结果直接批量插入到另一个表中
     * **`UPDATE`结合`CASE`**: ※※※“给不同工资水平的老师涨薪”的例子非常经典。如果用两个独立的`UPDATE`语句，需要非常小心执行顺序，否则可能会导致某些员工被涨薪两次。而使用一个`UPDATE`搭配`CASE`表达式，逻辑清晰且避免了潜在的错误。
   * **关键区分：** 清晰地辨析了`DELETE FROM table`和`DROP TABLE`的区别。前者清空表中的所有数据行，但表的结构（骨架）还在；后者是DDL，它直接从数据库中移除了整个表，连骨架都一起销毁了。这是“清空盒子”和“扔掉盒子”的根本区别。


**本周实践：**
本周的知识点实践性极强，且部分操作具有风险，我完成了本周的作业，将所学的DML和DDL知识应用到实际问题中。


---

### **第7-8周** （9分）

**本周核心内容：** 多表查询和join操作（中级SQL）

**总体感受：**
这几周的学习是SQL技能的一次“大升级”，学习了`JOIN`的各种类型和条件。

**具体收获与疑难点反思：**

1. **`JOIN`的系统学习**

   * **收获：** 我系统地学习了SQL中的各种`JOIN`操作，并理解了它们之间的区别：

     * **`INNER JOIN` (内连接):** 这是最常见的连接，等同于我们之前在`WHERE`子句中写的等值连接。它只返回两个表中能匹配上的行，任何一方没有匹配的行都会被丢弃。
     * **`OUTER JOIN` (外连接):** 这是本周的核心知识点。外连接是为了解决内连接“丢失数据”的问题而生的。

       * `LEFT JOIN`: 保留左表的所有行，右表中没有匹配的则用`NULL`填充。
       * `RIGHT JOIN`: 保留右表的所有行，左表中没有匹配的则用`NULL`填充。
       * `FULL JOIN`: 保留左右两表的所有行，相互没有匹配的都用`NULL`填充。
   * **易错点梳理：**

     * **`NATURAL JOIN`易错**: 例子`student NATURAL JOIN takes NATURAL JOIN course`的错误例子给我敲响了警钟。这个查询因为`student`表和`course`表都有`dept_name`列，导致`NATURAL JOIN`错误地添加了`student.dept_name = course.dept_name`的条件，这显然不是我想要的。这让我明白了`NATURAL JOIN`虽然简洁，但在复杂模式下是危险的，应当优先使用`JOIN ... USING(column)`或`JOIN ... ON(condition)`来明确指定连接条件。
     * **`ON` vs. `WHERE`易错**: 对于`INNER JOIN`，`ON`和`WHERE`在逻辑上是等价的。但对于`OUTER JOIN`，它们的行为完全不同：`ON`子句是**连接条件**，在生成临时表时用来匹配行；而`WHERE`子句是**过滤条件**，在连接完成后对结果进行筛选。课上的例子`LEFT JOIN ... ON true WHERE student.id = takes.id`会过滤掉所有`takes.id`为`NULL`的行（即未选课的学生），从而使`LEFT JOIN`退化成了`INNER JOIN`，这个例子让我彻底理解了二者的分工。

**本周实践：**
本周的`JOIN`是SQL的核心，我进行了如下练习：
1. 完成本周的作业，同时练习用`LEFT JOIN`解决“统计所有课程段（包括无人选课的）的选课人数”这类问题。
2. 将之前用隐式连接写的查询，尝试用显式的`JOIN ... ON ...`语法重写一遍，养成良好的编码习惯。
3. 尝试用`LEFT JOIN`、`RIGHT JOIN`和`UNION`来模拟`FULL OUTER JOIN`的功能，以加深对集合运算和外连接的理解。


---

### **第10周** (8.5分）

**本周核心内容：** 高级SQL (1)——高级数据类型、类型转换与授权管理

**总体感受：**
本周将我的视野从“如何操作数据”提升到了“如何更好地定义和管理数据”。学习了更精细的数据类型（如日期、时间）；接触了类型转换；最后学习了授权管理，初步体验了数据库管理员（DBA）的角色，思考如何保护数据的安全。这周的学习让我感觉到，一个设计优良的数据库，不仅在于查询逻辑的巧妙，更在于其底层数据定义的精准与安全机制的完善。

**具体收获与疑难点反思：**
**这之后的内容偏向于底层理论基础，加上临近期末时间不太充裕，可能部分内容不能完全理解，所以先记录梳理下来，后面继续学习**

1. **超越数字**

   * **收获：** 学习了SQL中的日期和时间类型（`DATE`, `TIME`, `TIMESTAMP`）。

     * **数据有效性：** 数据库会自动校验格式，杜绝了如`'2022-13-40'`这样的非法日期。
     * **计算能力：** 可以直接进行日期/时间运算，如使用`INTERVAL`进行加减，或用`EXTRACT`提取年份、月份等特定部分。这是字符串类型无法比拟的。
   * **实践：** 老师让我们查阅“如何计算两个日期的天数差”，我动手实践后发现，在PostgreSQL中，两个`date`类型的值可以直接相减得到一个代表天数的整数，非常方便。

2. **类型转换**

   * **总结：** 两种核心的类型转换方法：

     * **`CAST(expression AS type)`**: 这是SQL标准语法，通用性最强。
     * **`::` (PG特有语法)**: 写法更简洁，如`'3.14'::DECIMAL`。
     * **`to_xxx()`系列函数**: 像`to_date()`，它提供了强大的格式化模板，可以解析各种“奇形怪状”的日期字符串，在处理来自不同系统、格式不统一的数据时，这个功能会非常有用。

3. **授权管理**

   * **收获：** 第一次接触到了数据库的权限体系

     * **基本概念：** 理解了`GRANT`（授权）和`REVOKE`（撤销权限）的基本语法。
     * **角色** 亲手创建了`lilei`这个登录角色，并以他的身份尝试连接数据库、查询数据。
   * **安全意识：** 遵循**最小权限原则（Principle of Least Privilege）**，即只授予用户完成其工作所必需的最小权限，不多也不少。


**本周总结：**
本周的知识偏向于数据库的设计和管理，实践性和专业性很强，将来在实际业务需要的时候可以回顾这一章的内容。

---

### **第11周** （9.5分）

**本周核心内容：** 高级SQL ——可编程性数据库与应用安全

**总体感受：**
本周学习了程序如何与数据库交互，并直面了其中最致命的安全威胁——SQL注入。这周的学习让我深刻感受到，数据库应该不仅仅是一个数据仓库，还应该是一个强大的、可编程的应用后端。

**具体收获与疑难点反思：**

1. **触发器**

   * **收获：** 学习到可以自定义创建数据库逻辑单元，为我的期末项目提供了方法思路

     * **函数 (`FUNCTION`)**（可以将很多常用的、封装好的计算逻辑固化到数据库中，供所有查询复用，提高效率）
     * **过程 (`PROCEDURE`)**
     * **触发器 (`TRIGGER`)**
   * **心得：** 三者共同构成了数据库的“大脑”，让它从一个被动的数据存储系统，变成了一个能主动执行业务逻辑的智能系统。


2. **SQL注入（SQL Injection）**

   * **收获：** 彻底明白了SQL注入的原理和巨大危害。
   * **核心原理：** 攻击的本质是 **“将用户输入的数据，变成了可执行的SQL代码”**。经典例子：`pswd = "' or '1'='1"` 通过简单的字符串拼接，构造出`... AND pswd='' or '1'='1'`一条恒为真的恶意SQL，从而绕过密码验证。

    “**永远不要使用简单的字符串拼接来构造SQL语句**”

3.**课后实践**
    在LLM的帮助下学习了Github上一个项目程序实战案例（聊天机器人的Memory Module），这加深了我对非关系型数据库（Redis、MongoDB）在实际运用中的理解：

# 对接MongoDB：一个记忆系统的核心数据库操作指令

本文档总结了在一个基于图结构的记忆系统（源自`Hippocampus.py`）中，与MongoDB数据库进行交互所需的核心操作指令。这些指令覆盖了图的“节点”（Node）和“边”（Edge）的增、删、改、查（CRUD），以及在记忆构建过程中的相关操作。

## 1. 节点 (Node) 相关操作

### 查询所有节点

从 `nodes` 集合中检索所有节点文档。

```python
db_nodes = list(db.graph_data.nodes.find())
```

### 插入新节点

向 `nodes` 集合中插入一个新的节点文档。

```python
db.graph_data.nodes.insert_one(node_data)
```

### 更新节点

根据概念（`concept`）查找特定节点，并更新其关联的记忆项、哈希值和时间戳。

```python
db.graph_data.nodes.update_one(
    {"concept": concept},
    {"$set": {
        "memory_items": memory_items,
        "hash": memory_hash,
        "created_time": created_time,
        "last_modified": last_modified,
    }}
)
```

### 批量删除所有节点

清空整个 `nodes` 集合。

```python
db.graph_data.nodes.delete_many({})
```

---

## 2. 边 (Edge) 相关操作

### 查询所有边

从 `edges` 集合中检索所有边文档。

```python
db_edges = list(db.graph_data.edges.find())
```

### 插入新边

向 `edges` 集合中插入一条新的边文档。

```python
db.graph_data.edges.insert_one(edge_data)
```

### 更新边

根据源节点（`source`）和目标节点（`target`）查找特定边，并更新其哈希值、强度和时间戳。

```python
db.graph_data.edges.update_one(
    {"source": source, "target": target},
    {"$set": {
        "hash": edge_hash,
        "strength": strength,
        "created_time": created_time,
        "last_modified": last_modified,
    }}
)
```

### 批量删除所有边

清空整个 `edges` 集合。

```python
db.graph_data.edges.delete_many({})
```

---

## 3. 消息 (Message) 相关操作

此操作通常在记忆构建过程中使用，用于追踪原始消息被记忆的频率。

### 更新消息的记忆次数

根据消息的 `_id` 找到对应文档，并将其 `memorized_times` 字段加一。

```python
db.messages.update_one(
    {"_id": message["_id"]},
    {"$set": {"memorized_times": current_memorized_times + 1}}
)
```

---

## 4. 其他常见操作

### 更新节点/边的时间戳字段

在数据迁移或修复时，可能需要批量为缺少时间戳的文档补充字段。

```python
# 更新节点时间戳
db.graph_data.nodes.update_one({"concept": concept}, {"$set": update_data})

# 更新边的时间戳
db.graph_data.edges.update_one({"source": source, "target": target}, {"$set": update_data})
```

---

## 5. 批量操作 (重同步时)

在需要完全重置和同步图数据时，采用先清空后批量写入的策略。

### 清空并重写所有节点和边

```python
# 1. 清空集合
db.graph_data.nodes.delete_many({})
db.graph_data.edges.delete_many({})

# 2. 在循环中逐一写入所有新的节点和边数据
db.graph_data.nodes.insert_one(node_data)
db.graph_data.edges.insert_one(edge_data)
```

**注意**: 在生产环境中，对于大量数据，应优先考虑使用 `insert_many` 以提高性能。

---

## 6. 手动调试与查询

在开发和调试脚本中，经常需要查询单个节点或与其相关的边。

### 查询特定概念的节点数量

用于检查某个节点是否存在。

```python
db.graph_data.nodes.count_documents({"concept": concept})
```

### 查询与某节点相关的所有边

找到所有以该概念为源节点或目标节点的边。

```python
db.graph_data.edges.find({"$or": [{"source": concept}, {"target": concept}]})
```

---

### 总结

以上指令构成了记忆系统与MongoDB数据库交互的基础。它们通过 `pymongo` 驱动程序在Python代码中执行，实现了对图数据持久化存储的完整管理。

### 它补全了我在NoSQL和应用层数据管理上的知识拼图，让我明白数据库技术远不止SQL一种范式。真正的工程能力，是在理解不同工具的特性和局限后，为特定的业务场景选择最合适的解决方案，并用优秀的代码设计来弥补其短板、发挥其长处。

### 使用coze低代码工作流平台将数据查询功能整合到实际业务需求中：
![image](https://github.com/user-attachments/assets/44b64548-17c5-4a6c-885f-c0fc6644208d)

---

### **第12周学习总结与体会**（9分）

**本周核心内容：** 关系数据库范式（1NF-3NF）

**总体感受：**
本周学习了如何用 **函数依赖** 这一强大的数学工具来诊断这些“坏”的设计，并通过**模式分解**将其规范化，使其达到更优的范式（主要是BCNF和3NF）。

**具体收获与疑难点反思：**

#### **1. 冗余危害**

* **收获：** 不能简单地把所有属性都塞进一个大表里。例子：`ins_dept(ID, name, salary, dept_name, building, budget)`：

* **核心目标：** 好的数据库设计的核心目标就是**消除不必要的冗余**，同时保证数据检索的便捷性。而模式分解是实现这一目标的主要手段，但前提必须是**无损分解**，即分解后的表通过自然连接必须能完美地还原出原始信息。

---

#### 2. 函数依赖 

* **收获：** 理解了函数依赖`α → β`的本质：**在一张表中，只要属性（或属性集）`α`的值确定了，那么属性`β`的值也就唯一确定了。**

  * 例如，在`ins_dep`大表中，`ID → name`，`ID → salary`，`ID → dept_name`都成立。更重要的是，`dept_name → building`和`dept_name → budget`也成立。
* **平凡依赖：** 理解了`ID, name → name`这种“废话文学”式的依赖被称为平凡依赖，它没有提供任何新的信息。我们真正关心的是**非平凡依赖**。
* **无损分解判断：** 判定准则非常关键：对于`R`分解为`R1`和`R2`，只要`(R1 ∩ R2) → R1`或`(R1 ∩ R2) → R2`这两个函数依赖中有一个成立，那么这个分解就是无损的。对于`ins_dep`的分解，`instructor`和`department`的交集是`dept_name`，并且`dept_name → building, budget`（即`dept_name → department`），所以这个分解是无损的。

---

#### **3. BCNF **

* **收获：** 掌握了BCNF这一个非常严格且理想的范式。

  * **定义：** 对于关系模式`R`中任意一个**非平凡的**函数依赖`α → β`，`α`**必须是`R`的一个超码**。
  * **通俗理解：** 在一张表中，**任何一个属性都不能由非码属性来决定**。换句话说，决定因素`α`必须“足够大”，大到能决定整行记录。
* **思路整理：** `ins_dep`大表显然不满足BCNF，因为存在`dept_name → building, budget`，而`dept_name`并不是该表的超码（它不能决定`ID`, `name`等）。这正是我们需要分解的“病灶”。BCNF分解算法的思路就是：找到一个违反BCNF的`α → β`，然后将表拆分为`(α ∪ β)`和`(R - β)`。


---

### **第13-14周**（8.5分）

**核心内容：** 关系模式推导和E-R图设计

**总体感受：**
学习了如何从零开始，利用E-R模型这个工具将模糊的现实世界“需求”转化为清晰、严谨的数据库蓝图。

**具体收获与疑难点反思：**


#### **1. E-R模型的核心三要素：实体、属性、联系**

* **：** 梳理了E-R图的基本构成如下：

  * **实体 (Entity) / 实体集 (Entity Set)**
  * **属性 (Attribute)**: 实体的特征，整理的属性多种分类如下：

    * **复合属性 vs. 简单属性**: `name`可以分解为`first_name`, `last_name`，这就是复合属性。
    * **多值属性**: 一个人可以有多个电话号码`{phone_number}`，用花括号表示。
    * **派生属性**: `age()`可以由`date_of_birth`计算得出，用括号表示。派生属性通常不直接存储，以避免数据冗余和不一致。
  * **联系 (Relationship) / 联系集 (Relationship Set)**: 实体之间的关联，如图中的**菱形**（`advisor`）。联系也可以有自己的**描述性属性**，比如学生选课的联系`takes`上可以有`grade`（成绩）这个属性。
* **疑难点：** “为什么关系数据库要求属性原子性，但E-R图中却可以有复合和多值属性？” 

---

#### **2. 映射基数与参与约束**

* **收获：** 学会E-R模型中表达精确业务的规则。

  * **映射基数 (Mapping Cardinality)**: `One-to-One`, `One-to-Many`, `Many-to-Many`。例如，一个导师（`instructor`）可以指导多个学生（`student`），但一个学生只有一个导师，这就是一个`One-to-Many`的联系。

---

#### **3. 弱实体集 (Weak Entity Set)**

* **收获：** 后面补充理解了“弱实体集”的概念。它是一种自身没有足够属性形成主码、必须依赖于另一个“强实体集”才能唯一标识的实体。最经典的例子就是`section`（课程段）依赖于`course`（课程）。`section`的`sec_id`只是一个分辨符 ，它只有在与所依赖的`course`的`course_id`结合时，才能唯一确定一个课程段。在E-R图中，弱实体集和其标识性联系都用**双线边框**表示。

---


**本周实践：**

1. 较为熟练地掌握了E-R图的绘制规范，使用Draw.io等工具，按照作业要求了绘制E-R图。
2. 了解了E-R模型的替代品，如UML类图，了解它们在数据库设计中的异同。
例如，使用dbdiagram.io绘制智能家居系统的数据库表结构图（更接近实际应用）：
![image](https://github.com/user-attachments/assets/1a59a4aa-30a7-4676-933a-7e81d4c5dfa5)


---


### **第14周** （8.5分）

**核心内容：** 数据库底层——存储、索引与查询

**总体感受：**
本周从“如何使用”（SQL）和“如何设计”（E-R模型、范式）的层面，深入到了数据库系统的内部，这让我深刻理解了数据库之所以强大、高效和可靠的根本原因。这是一个从应用者到理解者的关键转变（正回应了第一周上课时引用的那句名言）。

**具体收获与疑难点反思：**

#### **1. 数据库存储**

* **收获：** 初步了解到数据是如何从内存持久化到物理磁盘上的，但时间不够充分实践消化理解，先记录在这儿，后续再学习。
* 内容整理（后续可以回顾该部分内容）

  * **Page（页）是基本单位：** 数据库并非随意地将数据写入文件，而是将文件划分为固定大小的`Page`（通常为4KB-16KB）来进行管理。这是为了优化与磁盘的I/O操作，因为磁盘I/O的“寻道时间”成本很高，一次性读写一个`Page`远比零散地读写几个字节要高效得多。
  * **Page的内部结构：** `Page`内部的“精装修”——通过`Slotted Page`（分页的槽结构）这种设计，巧妙地解决了变长记录（如`varchar`, `text`）的存储问题。通过一个槽数组（slot array）来记录每个元组的指针和长度，既实现了高效访问，又能灵活管理空间。
  * **物理元组ID (`CTID`)：** `CTID`这个“物理地址”，它由`Page ID`和`Slot ID`组成。这让我直观地理解了数据库是如何在物理层面精确定位到每一行记录的。
  * **行存储 vs. 列存储：** 这是一个非常重要的概念。我理解了它们分别适应的不同场景：

    * **行存储 (Row-Storage / NSM):** 将一整行数据连续存储，适合OLTP（在线事务处理）场景，如`SELECT * FROM user WHERE id = ?`，一次性获取某个实体的所有信息效率很高。
    * **列存储 (Column-Storage / DSM):** 将同一列的数据连续存储，适合OLAP（在线分析处理）场景，如`SELECT SUM(sales) FROM orders`，只需要读取`sales`这一列的数据，极大减少了不必要的I/O。
  * **数据字典 (Data Dictionary):** 数据库中存在一个“元数据”中心，它就像一个文件系统的目录，记录着哪个表对应哪个文件，以及其他重要的元信息，是DBMS进行查询的第一站。

---

#### **2. 索引**

* **收获：** 这是本周最核心、最实用的内容。我初步了解了索引是如何将查询性能提升几个数量级的，具体内容整理如下：

  * **核心权衡：** 索引的本质是\*\*“用空间换时间”\*\*。它需要额外的存储空间并会降低写操作（`INSERT`, `UPDATE`, `DELETE`）的性能，但能极大地加速读操作（`SELECT`）。
  * **B+树索引：** 我掌握了B+树这一最核心的索引结构。它是一种自平衡的树，保证了从根节点到任何叶子节点的路径长度都相同，从而实现了稳定的、数量级为 \$O(log N)\$ 的高效查找。课上那个百万级记录仅需访问少数几个节点的例子，让我对B+树的威力有了直观的感受。
  * **Hash索引：** 我也了解了另一种索引类型——哈希索引。它的优点是等值查询（`=`）速度极快，时间复杂度接近 \$O(1)\$，但缺点是完全不支持范围查询（`BETWEEN`, `>` 等）。课上那个用哈希索引优化`session_token`查询的例子非常贴切。
  * **重要概念辨析：**

    * **聚集索引 vs. 非聚集索引：** 关键区别：**聚集索引**决定了数据在磁盘上的物理存储顺序，因此**一个表只能有一个**。而非聚集索引只是一个独立的“目录”结构，指向物理数据，一个表可以有多个。
    * **稠密索引 vs. 稀疏索引：** 稠密索引为表中的每一条记录都创建一个索引项，而非聚集索引通常是稠密的。稀疏索引只为一部分记录（如每个`Page`的第一条记录）创建索引项，它必须建立在聚集索引（或物理有序的文件）之上。
---

#### **3. ACID**

* **收获：** 数据库区别于普通文件系统的核心优势——可靠性。

  * **定义：** 事务是一个“不可分割的逻辑工作单元”。

   * **ACID特性：** 记住了这四个数据一致性：

    * **原子性 (Atomicity):** 事务要么全部成功，要么全部失败回滚，不存在中间状态。
    * **一致性 (Consistency):** 事务使数据库从一个一致的状态转移到另一个一致的状态。
    * **隔离性 (Isolation):** 并发执行的事务之间互不干扰。
    * **持久性 (Durability):** 一旦事务提交，其结果就是永久性的。



---

### **第15周** （8分）

**核心内容：** 数据库系统实现（续）——查询优化、索引进阶与事务隔离

**总体感受：**
这两周的学习是在上周的基础上的进一步深化和拓展，面对实际业务场景。但是限于时间安排，后续有时间再具体实践，这里先梳理内容。

**具体收获与疑难点反思：**

#### **1. 查询处理与优化**

* **收获：** 我了解了一条SQL语句从被我们敲下到最终返回结果的完整流程：**解析 -> 优化 -> 执行**。其中，**查询优化器 (Query Optimizer)** 是整个过程的关键，它的核心工作就是生成一个最高效的**查询计划 (Query Plan)**。
* **例子：** 简单查询 `select salary from instructor where salary < 75000;` 可以被翻译成两种等价的关系代数表达式：

  1. `σ_salary<75000(Π_salary(instructor))` (先投影再选择)
  2. `Π_salary(σ_salary<75000(instructor))` (先选择再投影)
* **心得：** 显而易见，第二种方案要高效得多，因为它先通过`WHERE`条件过滤掉了大量数据，大大减少了后续投影操作需要处理的数据量。优化器的职责，就是基于数据统计信息（如表的大小、数据分布等），从成千上万种可能的执行路径中，选择成本最低的那一条。对`EXPLAIN`命令的输出有了更深层次的理解——它展示的正是优化器最终选定的那个“最优”计划。

---

#### **2. 索引的再思考**

* **收获：** 反思：**索引并非总是能提高性能**。
* **核心概念——选择性 (Selectivity) / 基数 (Cardinality):** 对于“gender”这种只有“男”、“女”两种取值的列（即低基数列），为其建立索引往往是**弊大于利**的。
* **感悟：** 索引最有效的场景是用于**高选择性**的列，即索引键的值能够过滤掉绝大部分数据（如`id`、`email`等唯一值列）。因此不能盲目地为所有`WHERE`条件中出现的列都加上索引。

---

#### **3. 事务的深化：ACID与隔离级别**
* **四大隔离级别（从低到高）：**
  1. **Read Uncommitted (读未提交):** 性能最好，但最不安全，会产生**脏读 (Dirty Read)**，即一个事务可能读到另一个事务尚未提交的、可能被回滚的数据。
  2. **Read Committed (读已提交):** 大多数数据库的默认级别。避免了脏读，但可能产生**不可重复读 (Non-Repeatable Read)**（同一事务内，两次读取同一行数据，结果不同）和**幻读 (Phantom Read)**（同一事务内，两次执行相同范围查询，结果集行数不同）。
  3. **Repeatable Read (可重复读):** 避免了脏读和不可重复读，但仍可能产生幻读。
  4. **Serializable (可串行化):** 最严格的级别，完全避免了上述所有问题，保证事务像串行执行一样。
 

---

### **第16周：** （9.5分）

* **核心内容：** DuckDB, 向量数据库 (Vector Database)

* **个人感受：** DuckDB和向量数据库的出现，说明数据库技术正朝着更加**专业化、场景化**的方向演进，以应对大数据分析和人工智能时代的独特挑战。

* **收获与反思：**

  1. **DuckDB**

     * **核心认知：**  DuckDB可以直接在SQL查询中引用Pandas DataFrame，也可以将查询结果轻松转回DataFrame。这克服了Python原生文件读取的效率瓶颈，后面或许能让我能用熟悉的SQL语法，以极高的性能处理本地的Parquet或CSV文件，如果能实现的话，可以加入。
   

       ```sql
       -- import duckdb
       -- import pandas as pd
       -- df = pd.DataFrame({'a': [1, 2, 3]})
       -- result = duckdb.sql("SELECT a * 2 FROM df").df()

       -- 查询外部文件
       CREATE TABLE reviews AS SELECT * FROM 'reviews.parquet';
       SUMMARIZE reviews;
       ```

  2. **向量数据库**
     * **学习实践：** 从CSDN学习了解`pgvector`插件后，让我意识到不一定需要一个全新的数据库系统来使用向量技术，现有的关系型数据库也在通过插件化的方式积极扩展自身的能力边界，例如：

       ```sql
       -- 首先，需要创建一个带有vector类型的列
       CREATE TABLE items (id SERIAL PRIMARY KEY, embedding vector(3));

       -- 插入向量数据
       INSERT INTO items (embedding) VALUES ('[1,2,3]'), ('[4,5,6]');

       -- 查询与向量[3,1,2]最相似的项 (使用L2距离)
       SELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;
       ```
     * **应用场景：** 向量数据库是当前大语言模型（LLM）应用中\*\*RAG（检索增强生成）\*\*模式的核心组件，它为AI模型提供了外部的、可更新的“长期记忆”，是解决模型幻觉、实现知识库问答的关键。
     * **其他实践** 在另一门课程中，我最开始尝试学习使用n8n自动化工作流平台搭建了一个使用pincone向量数据库的问答工作流，首先将文本数据（中国大百科全书.txt）embedding（如用text_embedding_3_small模型,chunk设置为100,overlap设置为0）后上传到云端向量数据库，后续利用**Webhook** 节点触发，接受用户的自然语言查询，再用embedding模型将query转为向量。后续或许可以改成**SQL查询的子流程**：将 **n8n** 的自动化能力、**agent** 的智能生成SQL与传统 **SQL** 数据库的结构化查询相结合，从而实现既能理解用户的自然语言意图，又能精确地从结构化数据源中拉取信息，应用到智能家居查询系统中，从而能实现更智能的NLP查询：
    ![image](https://github.com/user-attachments/assets/8d50cb07-165c-4096-a447-9893742ce68d)

    ![image](https://github.com/user-attachments/assets/36fad05b-9423-49bf-845f-f9c4e9ab85a7)

     
   ![image](https://github.com/user-attachments/assets/f14ca408-2757-4c03-8579-6a9716c161df)


  
















